{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8eCz8XiJyzx"
      },
      "source": [
        "# Word-splitting and text segmentation in East Asian languages\n",
        "\n",
        "As different as they are, Chinese, Japanese and Korean are lumped together as [CJK languages](https://en.wikipedia.org/wiki/CJK_characters) when discussed from an English-language point of view. One reason they're considered similar is that **spacing** is not used in the same way as in English. While analyzing English requires splitting sentences based on spaces, one difficulty for this language set (and others) is **determining where the breaks between words are.** In this section we will review how to use libraries to segment words in these languages as well as Thai and Vietnamese."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVESQ0miJyzy"
      },
      "source": [
        "<p class=\"reading-options\">\n",
        "  <a class=\"btn\" href=\"/text-analysis/splitting-words-in-east-asian-languages\">\n",
        "    <i class=\"fa fa-sm fa-book\"></i>\n",
        "    Read online\n",
        "  </a>\n",
        "  <a class=\"btn\" href=\"/text-analysis/notebooks/Splitting words in East Asian languages.ipynb\">\n",
        "    <i class=\"fa fa-sm fa-download\"></i>\n",
        "    Download notebook\n",
        "  </a>\n",
        "  <a class=\"btn\" href=\"https://colab.research.google.com/github/littlecolumns/ds4j-notebooks/blob/master/text-analysis/notebooks/Splitting words in East Asian languages.ipynb\" target=\"_new\">\n",
        "    <i class=\"fa fa-sm fa-laptop\"></i>\n",
        "    Interactive version\n",
        "  </a>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UB8A5mDJyzy"
      },
      "source": [
        "## Using libraries\n",
        "\n",
        "**Segmenting** is the process of splitting a text into separate words. There isn't always a \"right\" answer as to what the split should be, so you might have to **try a few different libraries** before you feel a good fit. The recommendations below aren't necessarily the best Python packages, they're just ones that had a bit of activity, seemingly-decent interfaces/documentation, and no external installs.\n",
        "\n",
        "### Using these libraries with scikit-learn\n",
        "\n",
        "After reading this page, you might want to learn how to use these libraries with scikit-learn vectorizers. In that case, check out tutorial on [how to make scikit-learn vectorizers work with Japanese, Chinese, and other East Asian languages page](/text-analysis/how-to-make-scikit-learn-natural-language-processing-work-with-japanese-chinese/) after you read this page.\n",
        "\n",
        "## Chinese: jieba\n",
        "\n",
        "The Chinese word segmentation library [jieba](https://github.com/fxsjy/jieba) is very popular when analyzing Chinese text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JaP7HX9Jyzy"
      },
      "outputs": [],
      "source": [
        "#!pip install jieba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EaS5T53Jyzz"
      },
      "source": [
        "Jieba has a few different techniques we can use to segment words. We can read the documentation to get into details, but one major question is whether we want the **smallest possible divisions.**\n",
        "\n",
        "Using `lcut` gives us individual words and what might be considered _noun phrases_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdWd5y8TJyzz",
        "outputId": "eaf3d06d-a923-41f2-c7c5-7368a27f0d83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['我', '来到', '北京', '清华大学']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import jieba\n",
        "\n",
        "jieba.lcut('我来到北京清华大学')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP4AEPjPJyzz"
      },
      "source": [
        "If we want to divide things up a bit more, we can add `cut_all=True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqIBMYocJyzz",
        "outputId": "e5cacdd2-9888-4cad-d57b-e1366318c2c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['我', '来到', '北京', '清华', '清华大学', '华大', '大学']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "jieba.lcut('我来到北京清华大学', cut_all=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKkZuvLrJyzz"
      },
      "source": [
        "The big difference is `cut_all` will split something like `清华大学` into both `清华` and `华大`.\n",
        "\n",
        "When might we use one compared to the other?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOKTYzBjJyzz"
      },
      "source": [
        "## Japanese: nagisa\n",
        "\n",
        "For Japanese, you'll be using the library [nagisa](https://github.com/taishi-i/nagisa)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha_T7e7dJyz0"
      },
      "outputs": [],
      "source": [
        "#!pip install nagisa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK5eozwPJyz0",
        "outputId": "6d1ea6c8-3c2a-4ea9-d031-623766136581"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Python', 'で', '簡単', 'に', '使える', 'ツール', 'です']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nagisa\n",
        "\n",
        "text = 'Pythonで簡単に使えるツールです'\n",
        "doc = nagisa.tagging(text)\n",
        "\n",
        "doc.words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WKzBh7HJyz0"
      },
      "source": [
        "In addition to simple tokenization, nagisa will also do **part-of-speech tagging**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69skwNDVJyz0",
        "outputId": "e6a41f31-8ae4-4466-9059-d495f301bd83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['名詞', '助詞', '形状詞', '助動詞', '動詞', '名詞', '助動詞']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words.postags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KhqOlTXJyz0"
      },
      "source": [
        "This allows you to do things like pluck out all of the nouns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnY6tKVTJyz0",
        "outputId": "02992572-ffbe-4404-ef77-0ce1d20f2b5a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Python', 'ツール']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nouns = nagisa.extract(text, extract_postags=['名詞'])\n",
        "nouns.words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCYA7GJNJyz0"
      },
      "source": [
        "## Korean: KoNLPy\n",
        "\n",
        "Korean does use spaces, but certain characters combine with the \"actual\" words, so you can't just split on spaces. [KoNPLy](http://konlpy.org/) has several engines that will help you with this. [See a comparison chart of the different engines here](http://konlpy.org/en/latest/morph/#performance-analysis) or [find more specific details](https://docs.google.com/spreadsheets/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rnYlhkvJyz0"
      },
      "outputs": [],
      "source": [
        "#!pip install konlpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sE4mzjSJyz0"
      },
      "outputs": [],
      "source": [
        "phrase = \"아버지가방에들어가신다\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3rgZkbaJyz0",
        "outputId": "0c64a1cb-4c94-4c07-9eb5-30c16acd4679"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['아버지가방에들어가', '이', '시ㄴ다']"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from konlpy.tag import Hannanum\n",
        "hannanum = Hannanum()\n",
        "hannanum.morphs(phrase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1KIj2eDJyz0",
        "outputId": "19da0b0a-9928-4370-d6e1-b8f1cdc14de9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['아버지', '가방', '에', '들어가', '시', 'ㄴ다']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "kkma.morphs(phrase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejk0MbjGJyz0",
        "outputId": "327770b9-e4a1-49c0-9735-32501f790c49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['아버지', '가방', '에', '들어가', '시', 'ㄴ다']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from konlpy.tag import Komoran\n",
        "komoran = Komoran()\n",
        "komoran.morphs(phrase)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy ekonlpy\n",
        "\n",
        "from konlpy.tag import Hannanum\n",
        "hannanum = Hannanum()\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "\n",
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "\n",
        "from konlpy.tag import Komoran\n",
        "komoran = Komoran()\n",
        "\n",
        "from ekonlpy.mecab import Mecab\n",
        "mecab = Mecab()"
      ],
      "metadata": {
        "id": "bBmKJvb-p4gy",
        "outputId": "173fbd7a-5205-4242-f8b5-7317f6c97e68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: ekonlpy in /usr/local/lib/python3.10/dist-packages (2.0.6)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.5.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (5.3.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.6 in /usr/local/lib/python3.10/dist-packages (from ekonlpy) (8.1.8)\n",
            "Requirement already satisfied: fugashi<2.0.0,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from ekonlpy) (1.4.0)\n",
            "Requirement already satisfied: mecab-ko-dic<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ekonlpy) (1.0.0)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from ekonlpy) (3.9.1)\n",
            "Requirement already satisfied: pandas<=2.2.3,>=1.5.3 in /usr/local/lib/python3.10/dist-packages (from ekonlpy) (2.2.2)\n",
            "Requirement already satisfied: scipy<=1.13.1,>1.10.0 in /usr/local/lib/python3.10/dist-packages (from ekonlpy) (1.13.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->ekonlpy) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->ekonlpy) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->ekonlpy) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<=2.2.3,>=1.5.3->ekonlpy) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<=2.2.3,>=1.5.3->ekonlpy) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<=2.2.3,>=1.5.3->ekonlpy) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<=2.2.3,>=1.5.3->ekonlpy) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install konlpy ekonlpy\n",
        "\n",
        "# from konlpy.tag import Hannanum\n",
        "# hannanum = Hannanum()\n",
        "\n",
        "# from konlpy.tag import Okt\n",
        "# okt = Okt()\n",
        "\n",
        "# from konlpy.tag import Kkma\n",
        "# kkma = Kkma()\n",
        "\n",
        "# from konlpy.tag import Komoran\n",
        "# komoran = Komoran()\n",
        "\n",
        "# from ekonlpy.mecab import Mecab\n",
        "# mecab = Mecab()\n",
        "\n",
        "# phrase = \"나만의\" # expected\n",
        "# phrase = \"프사를\" # expected\n",
        "\n",
        "phrase = \"생활의\" # expected 생활 without 의, but mecab finds XPN+NNG ---> need rule \"if starts with XPN and no Josa is found, check Okt result\"\n",
        "# phrase = \"생명\" # extra example, similar to above: all detect same: noun\n",
        "# phrase = \"생물\" # extra example, similar to above: all detect same: noun\n",
        "# phrase = \"생체\" # extra example, similar to above: all detect same: noun\n",
        "# phrase = \"초음속\" # extra example, similar to above: all detect same: noun\n",
        "# phrase = \"초능력\" # extra example, similar to above: all detect same: noun\n",
        "# phrase = \"무조건\" # extra example, similar to above: all detect same: 1 word (noun or mag)\n",
        "# phrase = \"불완전\" # extra example, similar to above: komoran finds XPN+noun, all others find noun\n",
        "# phrase = \"비상구\" # extra example, similar to above: all detect same: noun\n",
        "# phrase = \"무한\" # extra example, similar to above: all detect same: noun\n",
        "phrase = \"불신\" # \"noun\" with prefix, expect XPN+Noun: all detect same: noun (1 word)\n",
        "\n",
        "# phrase = \"안에서\" # expected\n",
        "# phrase = \"누구나\" # expected . All taggers find JX/J/Josa. But mecab finds NP+JX > NN rule not triggered, word is kept whole. Okt rule interferes!\n",
        "# phrase = \"나만의\" # expected 나만의 as 1 word\n",
        "# phrase = \"한번에\" # expects 한번에 as 1 word. All taggers find JKM or JKB or josa, would keep only 2 chars\n",
        "# phrase = \"나은\" # we whitelisted it, no rule can help? mecab,kkma,okt would keep as 1 word, komoran/hannanum would keep 1 char and remove 1 josa\n",
        "\n",
        "# from konlpy.tag import Hannanum\n",
        "# hannanum = Hannanum()\n",
        "# #hannanum.morphs(phrase) # to see morphemes\n",
        "print(f\"hannanum: {hannanum.pos(phrase)}\")\n",
        "\n",
        "# from konlpy.tag import Okt\n",
        "# okt = Okt()\n",
        "#okt.morphs(phrase)\n",
        "print(f\"okt: {okt.pos(phrase)}\")\n",
        "\n",
        "# from konlpy.tag import Kkma\n",
        "# kkma = Kkma()\n",
        "# #kkma.morphs(phrase)\n",
        "print(f\"kkma: {kkma.pos(phrase)}\")\n",
        "\n",
        "# from konlpy.tag import Komoran\n",
        "# komoran = Komoran()\n",
        "# #komoran.morphs(phrase)\n",
        "print(f\"komoran: {komoran.pos(phrase)}\")\n",
        "\n",
        "# from ekonlpy.mecab import Mecab\n",
        "# mecab = Mecab()\n",
        "# #komoran.morphs(phrase)\n",
        "print(f\"mecab: {mecab.pos(phrase)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQcfW_tZJ1iW",
        "outputId": "26b52cb8-08e4-428e-8321-b7af608620da"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hannanum: [('불신', 'N')]\n",
            "okt: [('불신', 'Noun')]\n",
            "kkma: [('불신', 'NNG')]\n",
            "komoran: [('불신', 'NNP')]\n",
            "mecab: [('불신', 'NNG')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PixdUupmJyz0"
      },
      "source": [
        "## Thai: tltk\n",
        "\n",
        "You can find [many Thai NLP packages here](https://github.com/kobkrit/nlp_thai_resources), but we'll focus on [tltk](https://pypi.org/project/tltk/). It doesn't have the best documentation and it might not be the most accurate, but it **doesn't require us to install anything extra** (e.g. Tensorflow) and that's the absolutely only reason why we're using it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRWUXkJWJyz1"
      },
      "outputs": [],
      "source": [
        "#!pip install tltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unxnciY3Jyz1",
        "outputId": "840260fe-b278-41b6-aae5-a191ff6ad656"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[('สำนักงาน', 'NOUN'),\n",
              "  ('เขต', 'NOUN'),\n",
              "  ('จตุจักร', 'PROPN'),\n",
              "  ('ชี้แจง', 'VERB'),\n",
              "  ('ว่า', 'SCONJ'),\n",
              "  ('<s/>', 'PUNCT')],\n",
              " [('ได้', 'AUX'),\n",
              "  ('นำ', 'VERB'),\n",
              "  ('ป้ายประกาศ', 'NOUN'),\n",
              "  ('เตือน', 'VERB'),\n",
              "  ('ปลิง', 'NOUN'),\n",
              "  ('ไป', 'VERB'),\n",
              "  ('ปัก', 'VERB'),\n",
              "  ('ตาม', 'ADP'),\n",
              "  ('แหล่งน้ำ', 'NOUN'),\n",
              "  (' \\n', 'NOUN'),\n",
              "  ('ใน', 'ADP'),\n",
              "  ('เขต', 'NOUN'),\n",
              "  ('อำเภอ', 'NOUN'),\n",
              "  ('เมือง', 'NOUN'),\n",
              "  ('<s/>', 'PUNCT')],\n",
              " [('จังหวัด', 'NOUN'),\n",
              "  ('อ่างทอง', 'PROPN'),\n",
              "  ('<s/>', 'PUNCT'),\n",
              "  ('หลังจาก', 'SCONJ'),\n",
              "  ('นาย', 'NOUN'),\n",
              "  ('สุ', 'PROPN'),\n",
              "  ('กิจ', 'NOUN'),\n",
              "  ('<s/>', 'PUNCT')],\n",
              " [('อายุ', 'NOUN'), ('<s/>', 'PUNCT')],\n",
              " [('65 ', 'NUM'), ('ปี', 'NOUN'), ('<s/>', 'PUNCT')],\n",
              " [('ถูก', 'AUX'),\n",
              "  ('ปลิง', 'VERB'),\n",
              "  ('กัด', 'VERB'),\n",
              "  ('แล้ว', 'ADV'),\n",
              "  ('ไม่ได้', 'AUX'),\n",
              "  ('ไป', 'VERB'),\n",
              "  ('พบ', 'VERB'),\n",
              "  ('แพทย์', 'NOUN'),\n",
              "  ('<s/>', 'PUNCT')]]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tltk\n",
        "\n",
        "phrase = \"\"\"สำนักงานเขตจตุจักรชี้แจงว่า ได้นำป้ายประกาศเตือนปลิงไปปักตามแหล่งน้ำ\n",
        "ในเขตอำเภอเมือง จังหวัดอ่างทอง หลังจากนายสุกิจ อายุ 65 ปี ถูกปลิงกัดแล้วไม่ได้ไปพบแพทย์\"\"\"\n",
        "\n",
        "pieces = tltk.nlp.pos_tag(phrase)\n",
        "pieces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXI1Tt80Jyz1"
      },
      "source": [
        "If you just want to split out everything individually, you'll need to jump through a tiny hoop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NWx8tqEJyz1",
        "outputId": "4a97d455-b5b8-4762-8db0-83d52eab4944"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('สำนักงาน', 'NOUN'), ('เขต', 'NOUN'), ('จตุจักร', 'PROPN'), ('ชี้แจง', 'VERB'), ('ว่า', 'SCONJ'), ('<s/>', 'PUNCT'), ('ได้', 'AUX'), ('นำ', 'VERB'), ('ป้ายประกาศ', 'NOUN'), ('เตือน', 'VERB'), ('ปลิง', 'NOUN'), ('ไป', 'VERB'), ('ปัก', 'VERB'), ('ตาม', 'ADP'), ('แหล่งน้ำ', 'NOUN'), (' \\n', 'NOUN'), ('ใน', 'ADP'), ('เขต', 'NOUN'), ('อำเภอ', 'NOUN'), ('เมือง', 'NOUN'), ('<s/>', 'PUNCT'), ('จังหวัด', 'NOUN'), ('อ่างทอง', 'PROPN'), ('<s/>', 'PUNCT'), ('หลังจาก', 'SCONJ'), ('นาย', 'NOUN'), ('สุ', 'PROPN'), ('กิจ', 'NOUN'), ('<s/>', 'PUNCT'), ('อายุ', 'NOUN'), ('<s/>', 'PUNCT'), ('65 ', 'NUM'), ('ปี', 'NOUN'), ('<s/>', 'PUNCT'), ('ถูก', 'AUX'), ('ปลิง', 'VERB'), ('กัด', 'VERB'), ('แล้ว', 'ADV'), ('ไม่ได้', 'AUX'), ('ไป', 'VERB'), ('พบ', 'VERB'), ('แพทย์', 'NOUN'), ('<s/>', 'PUNCT')]\n"
          ]
        }
      ],
      "source": [
        "words = [word for piece in pieces for word in piece]\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZyUV_RuJyz1"
      },
      "source": [
        "If you'd like to cast away the part of speech, just ask for the first part of the pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2XFtK4KJyz1",
        "outputId": "8354aaf0-1166-48d3-b897-3a591be5ffeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['สำนักงาน', 'เขต', 'จตุจักร', 'ชี้แจง', 'ว่า', '<s/>', 'ได้', 'นำ', 'ป้ายประกาศ', 'เตือน', 'ปลิง', 'ไป', 'ปัก', 'ตาม', 'แหล่งน้ำ', ' \\n', 'ใน', 'เขต', 'อำเภอ', 'เมือง', '<s/>', 'จังหวัด', 'อ่างทอง', '<s/>', 'หลังจาก', 'นาย', 'สุ', 'กิจ', '<s/>', 'อายุ', '<s/>', '65 ', 'ปี', '<s/>', 'ถูก', 'ปลิง', 'กัด', 'แล้ว', 'ไม่ได้', 'ไป', 'พบ', 'แพทย์', '<s/>']\n"
          ]
        }
      ],
      "source": [
        "words = [word[0] for piece in pieces for word in piece]\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oL6qZRiMJyz1"
      },
      "source": [
        "## Vietnamese: pyvi\n",
        "\n",
        "For Vietnamese we'll use [pyvi](https://github.com/trungtv/pyvi). There are [plenty of other options](https://github.com/undertheseanlp/NLP-Vietnamese-progress/blob/master/tasks/word_segmentation.md), but the best ones all involve installing Java and separate packages. We'll use pyvi to keep it simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9h1wEAPJyz1"
      },
      "outputs": [],
      "source": [
        "#!pip install pyvi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqQIt0i8Jyz1"
      },
      "source": [
        "Weirdly, when you run the `tokenize` method you get a string back..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHbyxX4qJyz1",
        "outputId": "2dc81262-70d2-43ee-fa24-80eaf69b150c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Trường đại_học bách_khoa hà_nội'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyvi import ViTokenizer, ViPosTagger\n",
        "\n",
        "words = ViTokenizer.tokenize(u\"Trường đại học bách khoa hà nội\")\n",
        "words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyFWlK3NJyz1",
        "outputId": "bfe8dd90-c816-4156-f86e-ff3936031024"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Trường', 'đại_học', 'bách_khoa', 'hà_nội']"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words = words.split(\" \")\n",
        "words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2FVH4bJJyz1"
      },
      "source": [
        "But! If you're also hunting for parts of speech, you end up with a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqylnalxJyz1",
        "outputId": "899fef6d-12c3-473b-d86a-3aef71aa6371"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['Trường', 'đại_học', 'Bách_Khoa', 'Hà_Nội'], ['N', 'N', 'Np', 'Np'])"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ViPosTagger.postagging(ViTokenizer.tokenize(u\"Trường đại học Bách Khoa Hà Nội\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u4BESr7Jyz1"
      },
      "source": [
        "You can split the words and parts of speech apart easily enough, if you need them in separate variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1CMpaQ2Jyz1",
        "outputId": "64fe5a2d-2a4d-4ddc-8d81-d2fdba46a75e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "words are ['Trường', 'đại_học', 'Bách_Khoa', 'Hà_Nội']\n",
            "pos are ['N', 'N', 'Np', 'Np']\n"
          ]
        }
      ],
      "source": [
        "words, pos = ViPosTagger.postagging(ViTokenizer.tokenize(u\"Trường đại học Bách Khoa Hà Nội\"))\n",
        "print('words are', words)\n",
        "print('pos are', pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8XZ2jSwJyz2"
      },
      "source": [
        "If you'd like them matched up (like in some of the examples above), you can use `zip` to pair the word and the part of speech."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX6qZ8frJyz2",
        "outputId": "4f0e189e-078f-408c-cc0b-858a58d92339"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Trường', 'N'), ('đại_học', 'N'), ('Bách_Khoa', 'Np'), ('Hà_Nội', 'Np')]"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(zip(words, pos))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fwteuynJyz2"
      },
      "source": [
        "## Review\n",
        "\n",
        "In this section we looked at **tokenizing** text in several different languages that can't just be split with spaces. To discover how to use these libraries with scikit-learn vectorizers, check out tutorial on [how to make scikit-learn vectorizers work with Japanese, Chinese, and other East Asian languages page](/text-analysis/how-to-make-scikit-learn-natural-language-processing-work-with-japanese-chinese/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT4Jya-1Jyz2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}